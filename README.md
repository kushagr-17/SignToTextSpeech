# SignToTextSpeech

A machine learning-powered application that translates sign language gestures into both text and speech, bridging communication gaps between the hearing and deaf communities.

# Overview

This project uses computer vision and machine learning to recognize sign language gestures and convert them into both spoken words and text in real-time. The system leverages deep learning models, OpenCV, and natural language processing (NLP) to provide an efficient and accessible solution for sign language users.

## Features

✅ Real-Time Gesture Recognition – Uses a webcam to detect hand signs.

✅ Text Output – Converts recognized gestures into readable text.

✅ Speech Output – Generates spoken words using text-to-speech (TTS) technology.

✅ Offline Functionality – Works without an internet connection after setup.

✅ Scalable & Modular – Designed for future enhancements like AI-powered sentence formation.


Technologies Used:

Python 🐍

OpenCV 👀 (for image processing)

TensorFlow/Keras 🤖 (for deep learning-based gesture recognition)

MediaPipe ✋ (for hand tracking)

NLTK/Text-to-Speech (TTS) 🎙️ (for speech output)

Flask 🌐 (for creating a web-based interface)
