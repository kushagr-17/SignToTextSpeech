# SignToTextSpeech

A machine learning-powered application that translates sign language gestures into both text and speech, bridging communication gaps between the hearing and deaf communities.

# Overview

This project uses computer vision and machine learning to recognize sign language gestures and convert them into both spoken words and text in real-time. The system leverages deep learning models, OpenCV, and natural language processing (NLP) to provide an efficient and accessible solution for sign language users.

## Features

âœ… Real-Time Gesture Recognition â€“ Uses a webcam to detect hand signs.

âœ… Text Output â€“ Converts recognized gestures into readable text.

âœ… Speech Output â€“ Generates spoken words using text-to-speech (TTS) technology.

âœ… Offline Functionality â€“ Works without an internet connection after setup.

âœ… Scalable & Modular â€“ Designed for future enhancements like AI-powered sentence formation.


Technologies Used:

Python ğŸ

OpenCV ğŸ‘€ (for image processing)

TensorFlow/Keras ğŸ¤– (for deep learning-based gesture recognition)

MediaPipe âœ‹ (for hand tracking)

NLTK/Text-to-Speech (TTS) ğŸ™ï¸ (for speech output)

Flask ğŸŒ (for creating a web-based interface)
